<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>tmux笔记 | 桂下闲谈</title><meta name="author" content="Haoyue Gui"><meta name="copyright" content="Haoyue Gui"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="【论文解读】（一）LGSNet: A Two-Stream Network for Micro- and Macro-Expression Spotting With Background ModelingWang-Wang Yu, Jingwen Jiang, Kai-Fu Yang, Hong-Mei Yan, and Yong-Jie Li University of Electronic">
<meta property="og:type" content="article">
<meta property="og:title" content="tmux笔记">
<meta property="og:url" content="http://ghy-cmd.github.io/2024/09/08/LGSNet%20A%20Two-Stream%20Network%20for%20Micro-%20and%20Macro-Expression%20Spotting%20With%20Background%20Modeling/index.html">
<meta property="og:site_name" content="桂下闲谈">
<meta property="og:description" content="【论文解读】（一）LGSNet: A Two-Stream Network for Micro- and Macro-Expression Spotting With Background ModelingWang-Wang Yu, Jingwen Jiang, Kai-Fu Yang, Hong-Mei Yan, and Yong-Jie Li University of Electronic">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://ghy-cmd.github.io/img/favicon_g-modified.png">
<meta property="article:published_time" content="2024-09-08T04:40:00.000Z">
<meta property="article:modified_time" content="2024-09-26T07:25:07.041Z">
<meta property="article:author" content="Haoyue Gui">
<meta property="article:tag" content="科研">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://ghy-cmd.github.io/img/favicon_g-modified.png"><link rel="shortcut icon" href="/img/favicon_g-modified.png"><link rel="canonical" href="http://ghy-cmd.github.io/2024/09/08/LGSNet%20A%20Two-Stream%20Network%20for%20Micro-%20and%20Macro-Expression%20Spotting%20With%20Background%20Modeling/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'tmux笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-26 15:25:07'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/modify.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/favicon_g-modified.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="桂下闲谈"><img class="site-icon" src="/img/favicon_g-modified.png"><span class="site-name">桂下闲谈</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">tmux笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-08T04:40:00.000Z" title="发表于 2024-09-08 12:40:00">2024-09-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-26T07:25:07.041Z" title="更新于 2024-09-26 15:25:07">2024-09-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="tmux笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="【论文解读】（一）LGSNet-A-Two-Stream-Network-for-Micro-and-Macro-Expression-Spotting-With-Background-Modeling"><a href="#【论文解读】（一）LGSNet-A-Two-Stream-Network-for-Micro-and-Macro-Expression-Spotting-With-Background-Modeling" class="headerlink" title="【论文解读】（一）LGSNet: A Two-Stream Network for Micro- and Macro-Expression Spotting With Background Modeling"></a>【论文解读】（一）LGSNet: A Two-Stream Network for Micro- and Macro-Expression Spotting With Background Modeling</h1><p>Wang-Wang Yu, Jingwen Jiang, Kai-Fu Yang, Hong-Mei Yan, and Yong-Jie Li</p>
<p>University of Electronic Science and Technology of China</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>未修剪视频中的微表情和宏表情检测是一项具有挑战性的任务，因为会大量产生假阳性样本。大多数现有方法通过从所有或部分关键原始图像中提取手工制作的特征或裁剪特定区域来定位更高响应区域，但这些方法要么忽略了连续的时间信息，要么将固有的人运动模式（背景）建模为前景。因此，我们提出了一种新颖的双流网络，名为局部抑制和全局增强检测网络（LGSNet），它以光流和视频的段级特征作为输入。LGSNet采用锚点来编码表情间隔，并选择编码偏差作为优化对象。此外，我们引入了时间多感受野特征融合模块（TMRF³M）和局部抑制与全局增强模块（LSGEM），这有助于更精确地检测短间隔并抑制背景信息。为了进一步突出正负样本之间的差异，我们在一些丢弃的滑动窗口上设置大量随机伪真值间隔（背景片段）来完成背景片段建模，以抵消无表情的面部和头部运动的影响。实验结果表明，我们提出的网络在CAS(ME)2、CAS(ME)3和SAMM - LV数据集上达到了最先进的性能。</p>
<h3 id="1-导论"><a href="#1-导论" class="headerlink" title="1. 导论"></a>1. 导论</h3><p>面部表情作为一种重要的非言语交流形式，能够不由自主或自愿地反映人类情绪的变化。它在极短的时间内传达诸如意图、选择、倾向等微妙的含义。表情可以分为宏表情（MaEs，也被称为常规或标准表情）和微表情。微表情通常在 0.5 秒内出现，具有非自愿性和低强度的特点。相比之下，宏表情（MaEs）是持续时间在 0.5 到 4 秒之间的明显面部变化。此外，微表情在面部的运动区域比宏表情小，有时微表情的左右面部动作单元（AUs）不对称，而宏表情的左右面部运动总是同步的。包括持续时间短和强度低在内的上述情况，是微表情难以捕捉的根本原因。而且，宏表情传达的情绪不一定真实，而当人们试图隐藏或抑制真实情绪时，微表情可以揭示真实意图。因此，微表情分析在心理诊断、刑事调查和商业谈判等高风险环境中具有价值，它弥补了传统表情分析的不足，特别是在识别谎言方面的困难。</p>
<p>面部表情分析包含两个部分：检测和识别。依据表情定义的类型，表情识别可被分类为基于离散标签的、基于连续模型的以及基于面部动作编码系统（FACS）的。 </p>
<ul>
<li><strong>基于离散标签的面部表情识别</strong>：旨在将表情定义为六种基本离散情绪，即快乐、惊讶、愤怒、厌恶、恐惧、悲伤，或者添加中性作为第七种类型，轻蔑作为第八种类型，并在一些传统数据集（如CK+、JAFFE、FER2013、Oulu - CASIA）上开展分类任务。此外，有些定义包含三种情绪类型（积极、消极和惊讶）或者四种类型（积极、消极、惊讶和其他）。 </li>
<li><strong>基于连续模型的面部表情识别</strong>：依照心理学的效价 - 唤醒圆环模型尝试将表情定义为两个连续值（或者添加优势作为第三个连续值），并通过在数据库（如SEWA、Aff - Wild2、AffectNet、AFEW - VA）上进行回归来完成此任务。基于连续值定义表情的这些细粒度方法正逐渐取代传统的离散标签方法。另一种常见的基于连续模型的方式是将表情定义为包括愉悦、唤醒和优势（PAD）的三个连续值。</li>
<li><strong>基于FACS的识别</strong>：试图将面部肌肉划分为动作单元（AUs），并将此任务转化为在特定数据集（如BP4D、DISFA）上检测AUs的问题。因此，宏表情识别通常基于单幅图像，采用直接分类或AU检测，也有使用视频进行回归和AU检测的趋势；而由于微表情强度低，微表情识别基于片段、关键帧。尽管保罗·埃克曼开发了微表情训练工具（METT）来帮助研究人员理解微表情的面部肌肉运动模式，但有经验的专家使用肉眼的准确率仍低于47%。在序列中检测AUs也可用于进行微表情分类。</li>
</ul>
<blockquote>
<p>心理学的效价 - 唤醒圆环模型（valence-arousal circumplex model）是一种用于描述和理解情绪的理论模型。在面部表情识别等相关研究中具有重要应用： </p>
<ul>
<li><strong>主要概念</strong><ul>
<li><strong>效价（valence）</strong>：指情绪的积极或消极程度。积极情绪具有正效价，如快乐、愉悦等；消极情绪具有负效价，如悲伤、愤怒、恐惧等。它反映了个体对情绪体验的评价，即情绪是令人愉悦的还是不愉悦的。</li>
<li><strong>唤醒（arousal）</strong>：表示情绪的激活或兴奋程度。高唤醒水平意味着个体处于兴奋、激动的状态，例如在激烈运动或面临紧急情况时；低唤醒水平则表示个体较为平静、放松，比如在安静休息或处于熟悉、舒适的环境中。</li>
</ul>
</li>
<li><strong>模型结构</strong>：该模型将效价和唤醒作为两个维度，构建出一个圆环形状的空间。情绪在这个空间中可以用一个坐标点来表示，不同的情绪位于圆环上的不同位置。例如，快乐通常被认为是高积极效价和高唤醒水平的情绪，可能位于圆环的右上部分；而悲伤则是高消极效价和低唤醒水平的情绪，可能位于圆环的左下部分。</li>
<li><strong>应用领域</strong> <ul>
<li><strong>情绪研究</strong>：帮助心理学家更系统地研究和分类情绪，理解不同情绪之间的关系和差异。通过确定情绪在效价 - 唤醒圆环上的位置，可以对情绪进行量化和比较。</li>
<li><strong>面部表情识别</strong>：在面部表情分析中，基于连续模型的面部表情识别尝试根据该模型将表情定义为两个连续值（效价和唤醒），或者添加优势（dominance）作为第三个连续值。通过分析面部表情所对应的效价和唤醒程度，可以对表情进行更细致的识别和分类。这对于理解人类情绪表达、情感计算等领域具有重要意义，例如在人机交互、心理诊断、情感分析等方面有应用价值，有助于开发更准确的情绪识别系统和更好地理解人类情感状态。</li>
</ul>
</li>
</ul>
</blockquote>
<p>检测作为一项先行任务，旨在识别面部出现的连续运动是习惯性运动、宏表情还是微表情。由于表情的持续时间不一致，在稀疏的长视频中检测微表情或宏表情片段比识别要困难得多。因此，早期的数据集（SAMM、SMIC、CASME和CASME II）主要关注识别。直到CAS(ME)2、SAMM - LV和CAS(ME)3提供了包含宏表情和微表情标注间隔的长视频，才有了一些关于检测的相关工作。在实践中，微表情和宏表情之间的显著差异是持续时间不一致，这在长视频中标注间隔时是最重要的基准。然而，这种标注方法总是耗时费力的，因为两个或更多的标注者需要花费大量时间逐帧多次浏览所有视频来确定表情间隔的位置。在这种情况下，用自动检测方法取代人工标注非常值得研究。 </p>
<p>这项工作旨在设计一种高效的自动表情检测方法。因为表情是动态且连续的，它包含起始帧、顶点帧和偏移帧来展示情绪的完整变化。起始帧是表情的起点，通常具有明显的面部突变。顶点帧显示面部肌肉变化的峰值，以便容易地传达情感信息。偏移帧定义了表情结束的时间。所以检测任务的本质是尽可能多地定位有效提案的关键帧（通常是中心），并同时确定每个提案的持续时间，从而使真实区间和所选提案之间的交并比（IoU）保持在指定阈值以上。</p>
<p>传统的检测方法假设在没有表情时面部没有大的变形，并旨在在固定的时间尺度内计算差异，例如局部二值模式（LBP）、方向梯度直方图（HOG）和光流。然而，表情的尺度变化极大。例如，CAS (ME) 2 数据集上最长的间隔比最短的间隔长 10 倍以上。因此，固定的时间尺度只能检测到很少的表情。一种直观的方法是采用多尺度持续时间，但由于存在一般的习惯性动作（如眨眼、撅嘴、摇头等），这反过来会产生大量的负样本。这要求我们基于更多的尺度变化覆盖尽可能多的表情，同时减少负样本。</p>
<p>当前基于深度学习的检测方法有两种类型：基于帧的方法和基于片段的方法。</p>
<ul>
<li><strong>基于帧的方法</strong>：基于对特定帧的判断。例如Pan等人提出基于CNN的模型将所有帧分类为宏表情、微表情或无关帧，其假设微表情和宏表情相互排斥，但实际中它们常分布在同一长视频中且面部运动相对相似；Yap等人基于对所有样本持续时间的计数进行了长短跳帧的修改。</li>
<li><strong>基于片段的方法</strong>：大多基于长短期记忆（LSTM）网络，利用离散图像的下采样特征或区域作为输入；</li>
<li>其他的采用类似于片段提议网络的策略，即有效编码方案能高效编码所有样本并过滤负样本。</li>
</ul>
<blockquote>
<p>长短期记忆（LSTM）网络的结构主要包含以下部分及相关公式：</p>
<h3 id="细胞状态（Cell-State）"><a href="#细胞状态（Cell-State）" class="headerlink" title="细胞状态（Cell State）"></a>细胞状态（Cell State）</h3><ul>
<li>细胞状态$C_t$是LSTM的核心，用于存储长期信息。它在时间步$t$的更新基于遗忘门、输入门和前一时刻的细胞状态$C_{t - 1}$以及新的候选细胞状态$\tilde{C}_t$。<ul>
<li><strong>更新公式</strong>：$C_t = f_t\odot C_{t - 1}+i_t\odot\tilde{C}_t$<ul>
<li>其中，$\odot$表示逐元素相乘。$f_t$是遗忘门的输出，控制从$C_{t - 1}$中遗忘的信息；$i_t$是输入门的输出，决定新的信息$\tilde{C}_t$有多少被添加到细胞状态中；$\tilde{C}_t$由输入数据和当前隐藏状态等计算得到。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="门控机制（Gating-Mechanism）"><a href="#门控机制（Gating-Mechanism）" class="headerlink" title="门控机制（Gating Mechanism）"></a>门控机制（Gating Mechanism）</h3><ul>
<li><strong>遗忘门（Forget Gate）</strong><ul>
<li><strong>结构和计算</strong>：遗忘门用于决定从细胞状态中丢弃哪些信息。它根据当前输入$x_t$和上一时刻的隐藏状态$h_{t - 1}$进行计算，通过一个sigmoid函数将输出值压缩在0到1之间。</li>
<li><strong>公式</strong>：$f_t=\sigma(W_f\cdot[x_t,h_{t - 1}]+b_f)$<ul>
<li>其中，$\sigma$是sigmoid函数，$W_f$是遗忘门的权重矩阵，$b_f$是偏置项。$f_t$的值越接近0，表示越多的信息将被遗忘；越接近1，表示越多的信息将被保留。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输入门（Input Gate）</strong><ul>
<li><strong>结构和计算</strong>：输入门由两部分组成，一是决定哪些值需要更新的sigmoid层，二是生成新的候选值向量的tanh层。</li>
<li><strong>公式</strong><ul>
<li>首先，sigmoid层：$i_t=\sigma(W_i\cdot[x_t,h_{t - 1}]+b_i)$，这里$i_t$控制更新的程度，与遗忘门类似，值在0到1之间。</li>
<li>其次，tanh层：$\tilde{C}<em>t=\tanh(W_C\cdot[x_t,h</em>{t - 1}]+b_C)$，生成新的候选细胞状态，$\tanh$函数将值映射到 -1到1之间。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输出门（Output Gate）</strong><ul>
<li><strong>结构和计算</strong>：输出门决定细胞状态的哪些信息将被输出作为当前时间步的隐藏状态$h_t$。它根据当前输入$x_t$和上一时刻的隐藏状态$h_{t - 1}$计算。</li>
<li><strong>公式</strong><ul>
<li>首先，$o_t=\sigma(W_o\cdot[x_t,h_{t - 1}]+b_o)$，其中$o_t$是输出门的输出，通过sigmoid函数计算得到，控制输出的比例。</li>
<li>然后，$h_t = o_t\odot\tanh(C_t)$，将细胞状态$C_t$经过$\tanh$函数处理后与输出门的输出$o_t$相乘，得到最终的隐藏状态$h_t$，作为当前时间步的输出，传递到下一个时间步或用于后续的任务（如分类、预测等）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>在这些公式中，$W_f$、$W_i$、$W_C$、$W_o$分别是遗忘门、输入门（包括sigmoid层和tanh层中与输入相关的部分）、候选细胞状态计算、输出门的权重矩阵，$b_f$、$b_i$、$b_C$、$b_o$分别是相应的偏置项。这些权重和偏置在训练过程中通过反向传播算法进行学习和调整，以优化LSTM网络对时间序列数据的处理能力和预测准确性。</p>
<p>例如，在处理一个时间序列预测任务时，LSTM通过这些门控机制和细胞状态的更新，逐步学习到数据中的时间依赖关系。在每个时间步，输入数据进入网络，遗忘门决定上一时刻细胞状态中哪些信息不再重要需要遗忘，输入门决定当前输入中有多少新的信息可以添加到细胞状态中，新的候选细胞状态根据输入和隐藏状态计算得到，然后与遗忘门和输入门的输出一起更新细胞状态。最后，输出门根据当前状态决定输出哪些信息作为隐藏状态，用于后续的计算或预测。这种结构使得LSTM能够有效地处理长期依赖关系，在许多时间序列相关的任务中取得了良好的效果，如自然语言处理中的语言建模、机器翻译，以及时间序列预测中的股票价格预测、气象预测等领域。</p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/0911f773e8cf196f8eb2c386435c3c8e.png" alt="img"></p>
</blockquote>
<p>然而，微表情的训练样本仍然不足，因为微表情难以收集。因此，当前基于深度学习的方法仍依赖传统手工特征或采样视频图像的多次下采样特征，忽略了原始视频的时间信息。例如，由于微表情的面部强度很微妙，多次下采样输入图像会丢失运动信息。传统手工特征往往只表征部分重要特征，丢失了微妙运动信息。即使使用多个手工特征作为输入，结果仍不理想。许多改进的传统方法增强局部面部运动以定位更多区域，但带来了更多假阳性区域。我们更倾向于从原始视频序列中提取外观信息，从原始视频的光流中生成运动信息。使用预训练模型基于原始视频序列提取特征能尽可能保留关键的时空信息。</p>
<p>如上所述，当前工作很少关注对负样本的编码，大多数负样本与习惯性头部或面部运动有关（本文中背景间隔和负样本基本对应）。同时，在训练阶段，主流方法最小化真实区间和预测之间的帧级差异，给长视频中的判别部分更高置信度。然而，那些习惯性动作也会产生大的帧间差异。即使依靠先验知识设置特定帧间隔来减少上述影响，也不能覆盖不同人习惯性动作的持续时间。而且，一些面部变化趋于缓慢且持续时间长。当仅检测到显著面部变化时，预定义的帧间隔会导致更少的正样本和更多的假阳性。 </p>
<p>我们还注意到，生成短持续时间的有效提案（或间隔）更依赖于关键帧的精确定位，而生成长间隔的有效提案更依赖于持续时间的长度。然而，当前的方法很少关注细化较短间隔定位的问题。这也导致现有微表情检测方法的稳健性较差。直接原因是一旦关键帧的短提案定位有较大偏差，其与真实区间的交并比（IoU）往往趋于0，即使其持续时间被精确预测。相比之下，长提案即使与真实区间有较大偏差，只要保持合适的持续时间，仍能与真实区间保持IoU大于0.5。例如，如果将真实区间编码为一系列关键帧和持续时间，当滑动窗口长度为128帧时，对于那些短间隔（如小于20帧），学习更倾向于实现准确的关键帧定位而非预测合适的持续时间，以保持提案与真实区间的高IoU。</p>
<p>SAMM - LV数据集的一些较长真实区间持续时间超过4秒，有些甚至超过10秒。这种分布显然与埃克曼关于微表情持续时间小于0.5秒、宏表情持续时间在0.5到4秒之间的观察不一致。为了减弱这种分布的影响，首先，我们丢弃不符合现实分布的长真实区间以减少噪声数据的影响。为了减少相邻帧的重叠和数据集之间的差异，也有必要对当前数据进行下采样来重构SAMM - LV数据集。 </p>
<p>据我们所知，这项工作是首次尝试提出一个双流框架，即局部抑制和全局增强检测网络（LGSNet），采用新的检测编码公式来解决上述问题。我们应用先验多级锚点来编码表情区间的中心和持续时间，利用预定义的IoU阈值过滤编码偏差并将过滤后的偏差作为优化目标。在训练期间，我们间接细化偏差和与目标区间对应的置信度。为了进一步整合多尺度特征，设计了时间多感受野特征融合模块（TMRF³M）来增强较短片段的信息。此外，设计了局部抑制和全局增强模块（LSGEM）来关注精确位置并限制较短样本的数量。在生成滑动窗口时，我们在一些废弃的滑动窗口上随机设置伪标签作为辅助数据集来实现背景间隔建模以细化定位和改进分类。为了在不丢弃外观信息的情况下保留面部运动，我们将从原始图像和光流中提取的重叠段级时空特征连接起来作为输入。来自光流的段级时间特征也可以捕获微妙的局部运动。此外，我们通过去除不现实的噪声数据并下采样到与CAS(ME)相同的FPS来重构SAMM - LV数据集。设计了一个名为具有一定k的top_threshold的改进指标以实现更合理的评估。实验结果表明我们的LGSNet在CAS(ME)2、CAS(ME)3和SAMM - LV上取得了更好的结果。我们的贡献总结如下：</p>
<ol>
<li>我们探索了对表情区间的中心和持续时间进行编码并使用预定义锚点分配标签，通过间接最小化偏差而非帧间差异大大降低了模型的学习难度。</li>
<li>提出的TMRF³M和LSGEM可以从全局信息中捕获关键局部部分，用于细化较短间隔的检测。</li>
<li>我们初步证明了来自视频和光流的段级特征作为输入可以大大提高表情检测的预测。</li>
<li>为了区分正负样本的差异，我们对一些丢弃的滑动窗口进行采样并设置伪标签来模拟背景运动。</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://ghy-cmd.github.io">Haoyue Gui</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://ghy-cmd.github.io/2024/09/08/LGSNet%20A%20Two-Stream%20Network%20for%20Micro-%20and%20Macro-Expression%20Spotting%20With%20Background%20Modeling/">http://ghy-cmd.github.io/2024/09/08/LGSNet%20A%20Two-Stream%20Network%20for%20Micro-%20and%20Macro-Expression%20Spotting%20With%20Background%20Modeling/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://ghy-cmd.github.io" target="_blank">桂下闲谈</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">科研</a></div><div class="post_share"><div class="social-share" data-image="/img/favicon_g-modified.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2024/09/08/tmux%E7%AC%94%E8%AE%B0/" title="tmux笔记"><img class="cover" src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRsH2UuE1jk0KpmdNU6oIfHhFERazqbmaymcA&amp;s" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">tmux笔记</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/favicon_g-modified.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">Haoyue Gui</div><div class="author-info__description">Nothing is impossible.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ghy-cmd" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:ghybuaa@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%80%90%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E3%80%91%EF%BC%88%E4%B8%80%EF%BC%89LGSNet-A-Two-Stream-Network-for-Micro-and-Macro-Expression-Spotting-With-Background-Modeling"><span class="toc-number">1.</span> <span class="toc-text">【论文解读】（一）LGSNet: A Two-Stream Network for Micro- and Macro-Expression Spotting With Background Modeling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.0.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AF%BC%E8%AE%BA"><span class="toc-number">1.0.2.</span> <span class="toc-text">1. 导论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%86%E8%83%9E%E7%8A%B6%E6%80%81%EF%BC%88Cell-State%EF%BC%89"><span class="toc-number">1.0.3.</span> <span class="toc-text">细胞状态（Cell State）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E6%9C%BA%E5%88%B6%EF%BC%88Gating-Mechanism%EF%BC%89"><span class="toc-number">1.0.4.</span> <span class="toc-text">门控机制（Gating Mechanism）</span></a></li></ol></li></ol></div></div><div class="card-widget card-post-series"><div class="item-headline"><i class="fa-solid fa-layer-group"></i><span>系列文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/08/LGSNet%20A%20Two-Stream%20Network%20for%20Micro-%20and%20Macro-Expression%20Spotting%20With%20Background%20Modeling/" title="tmux笔记">tmux笔记</a><time datetime="2024-09-08T04:40:00.000Z" title="发表于 2024-09-08 12:40:00">2024-09-08</time></div></div></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/09/08/tmux%E7%AC%94%E8%AE%B0/" title="tmux笔记"><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRsH2UuE1jk0KpmdNU6oIfHhFERazqbmaymcA&amp;s" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="tmux笔记"></a><div class="content"><a class="title" href="/2024/09/08/tmux%E7%AC%94%E8%AE%B0/" title="tmux笔记">tmux笔记</a><time datetime="2024-09-08T04:40:00.000Z" title="发表于 2024-09-08 12:40:00">2024-09-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/08/LGSNet%20A%20Two-Stream%20Network%20for%20Micro-%20and%20Macro-Expression%20Spotting%20With%20Background%20Modeling/" title="tmux笔记">tmux笔记</a><time datetime="2024-09-08T04:40:00.000Z" title="发表于 2024-09-08 12:40:00">2024-09-08</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2020 - 2024 By Haoyue Gui</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>